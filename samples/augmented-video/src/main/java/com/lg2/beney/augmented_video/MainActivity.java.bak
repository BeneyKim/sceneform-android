package com.lg2.beney.augmented_video;

import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.media.Image;
import android.media.MediaPlayer;
import android.net.Uri;
import android.os.Bundle;
import android.util.Log;
import android.view.ViewGroup;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;
import androidx.appcompat.widget.Toolbar;
import androidx.core.util.Pair;
import androidx.core.view.ViewCompat;
import androidx.core.view.WindowInsetsCompat;
import androidx.fragment.app.Fragment;
import androidx.fragment.app.FragmentManager;
import androidx.fragment.app.FragmentOnAttachListener;

import com.google.android.filament.Engine;
import com.google.android.filament.filamat.MaterialBuilder;
import com.google.android.filament.filamat.MaterialPackage;
import com.google.ar.core.AugmentedImage;
import com.google.ar.core.AugmentedImageDatabase;
import com.google.ar.core.CameraConfig;
import com.google.ar.core.CameraConfigFilter;
import com.google.ar.core.Config;
import com.google.ar.core.Frame;
import com.google.ar.core.ImageFormat;
import com.google.ar.core.Session;
import com.google.ar.core.TrackingState;
import com.google.ar.core.exceptions.CameraNotAvailableException;
import com.google.ar.sceneform.AnchorNode;
import com.google.ar.sceneform.FrameTime;
import com.google.ar.sceneform.Sceneform;
import com.google.ar.sceneform.math.Quaternion;
import com.google.ar.sceneform.math.Vector3;
import com.google.ar.sceneform.rendering.EngineInstance;
import com.google.ar.sceneform.rendering.ExternalTexture;
import com.google.ar.sceneform.rendering.Material;
import com.google.ar.sceneform.rendering.ModelRenderable;
import com.google.ar.sceneform.rendering.Renderable;
import com.google.ar.sceneform.rendering.RenderableInstance;
import com.google.ar.sceneform.ux.ArFragment;
import com.google.ar.sceneform.ux.BaseArFragment;
import com.google.ar.sceneform.ux.InstructionsController;
import com.google.ar.sceneform.ux.TransformableNode;
import com.google.gson.Gson;
import com.google.gson.reflect.TypeToken;
import com.lg2.beney.augmented_video.core.EdgeDetector;
import com.lg2.beney.augmented_video.core.ObjectDetector;
import com.lg2.beney.augmented_video.core.VideoHash;
import com.lg2.beney.augmented_video.utils.ImageUtils;
import com.lg2.beney.augmented_video.utils.SnackbarHelper;
import com.lg2.beney.augmented_video.utils.TypeConvertor;

import org.opencv.android.OpenCVLoader;
import org.opencv.calib3d.Calib3d;
import org.opencv.core.Core;
import org.opencv.core.CvException;
import org.opencv.core.CvType;
import org.opencv.core.Mat;
import org.opencv.core.MatOfInt;
import org.opencv.core.MatOfPoint;
import org.opencv.core.MatOfPoint2f;
import org.opencv.core.Point;
import org.opencv.core.RotatedRect;
import org.opencv.core.Scalar;
import org.opencv.core.Size;
import org.opencv.img_hash.ImgHashBase;
import org.opencv.img_hash.PHash;
import org.opencv.imgproc.Imgproc;

import java.io.BufferedReader;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.lang.reflect.Type;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Comparator;
import java.util.List;
import java.util.Optional;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;

public class MainActivity extends AppCompatActivity implements
        FragmentOnAttachListener,
        BaseArFragment.OnSessionConfigurationListener {

    private final String LOG_TAG = this.getClass().getSimpleName();

    private final List<CompletableFuture<Void>> futures = new ArrayList<>();
    private ArFragment arFragment;
    private boolean matrixDetected = false;
    private boolean rabbitDetected = false;
    private boolean math2Detected = false;
    private boolean math4Detected = false;
    private AugmentedImageDatabase database;
    private Renderable plainVideoModel;
    private Material plainVideoMaterial;
    private MediaPlayer mediaPlayer;

    private final SnackbarHelper snackbarHelper = new SnackbarHelper();

    private final String AR_IMAGE_DATABASE_FILENAME = "ar_image_database.imgdb";
    // private final String AR_IMAGE_DATABASE_FILENAME = "error.imgdb";
    private final String AR_IMAGE_DATABASE_JSON_FILENAME = "ar_image_database.json";

    private List<ArImageSet> mArImageSet = null;

    private ObjectDetector mObjectDetector = null;
    private EdgeDetector mEdgeDetector = null;
    private VideoHash mVideoHash = null;

    static {
        if (!OpenCVLoader.initDebug()) {
            // Handle initialization error
        }
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        setContentView(R.layout.activity_main);
        Toolbar toolbar = findViewById(R.id.toolbar);
        setSupportActionBar(toolbar);
        ViewCompat.setOnApplyWindowInsetsListener(toolbar, (v, insets) -> {
            ((ViewGroup.MarginLayoutParams) toolbar.getLayoutParams()).topMargin = insets
                    .getInsets(WindowInsetsCompat.Type.systemBars())
                    .top;

            return WindowInsetsCompat.CONSUMED;
        });
        getSupportFragmentManager().addFragmentOnAttachListener(this);

        if (savedInstanceState == null) {
            if (Sceneform.isSupported(this)) {
                getSupportFragmentManager().beginTransaction()
                        .add(R.id.arFragment, ArFragment.class, null)
                        .commit();
            }
        }

        if(Sceneform.isSupported(this)) {
            // .glb models can be loaded at runtime when needed or when app starts
            // This method loads ModelRenderable when app starts
            loadMatrixModel();
            loadMatrixMaterial();
        }

        mIntermediateMat = new Mat();
        hierarchy = new Mat();

        mObjectDetector = new ObjectDetector(this);
        mEdgeDetector = new EdgeDetector();
        mVideoHash = new VideoHash();
    }

    @Override
    public void onAttachFragment(@NonNull FragmentManager fragmentManager, @NonNull Fragment fragment) {
        if (fragment.getId() == R.id.arFragment) {
            arFragment = (ArFragment) fragment;
            arFragment.setOnSessionConfigurationListener(this);
        }
    }

    @Override
    public void onSessionConfiguration(Session session, Config config) {
        // Disable plane detection
        config.setPlaneFindingMode(Config.PlaneFindingMode.DISABLED);

        // Camera Resolution Up
        try {
            CameraConfigFilter filter = new CameraConfigFilter(session).setFacingDirection(CameraConfig.FacingDirection.BACK);
            List<CameraConfig> configs = new ArrayList<>(session.getSupportedCameraConfigs(filter));
            //Log.d(TAG, "[For Debug] CameraConfig Size: " + configs.size());
            configs.sort((n1, n2) -> {
                if (n1.getImageSize().getWidth() != n2.getImageSize().getWidth()) {
                    return n2.getImageSize().getWidth() - n1.getImageSize().getWidth();
                }
                return n2.getImageSize().getHeight() - n1.getImageSize().getHeight();
            });
            //Log.d(TAG, "[For Debug] Camera Config - getWidth: " + configs.get(0).getImageSize().getWidth() + " ,getHeight: " + configs.get(0).getImageSize().getHeight() + " ,getFpsRange: "  + configs.get(0).getFpsRange() + " ,getCameraId: " + configs.get(0).getCameraId());
            session.setCameraConfig(configs.get(0));

            session.resume();
            session.pause();
            session.resume();

        } catch (CameraNotAvailableException e) {
            // Ignore
        }

        // Images to be detected by our AR need to be added in AugmentedImageDatabase
        // This is how database is created at runtime
        // You can also prebuild database in you computer and load it directly (see: https://developers.google.com/ar/develop/java/augmented-images/guide#database)

        try (InputStream is = getAssets().open(AR_IMAGE_DATABASE_FILENAME)) {
            database = AugmentedImageDatabase.deserialize(session, is);

        } catch (Exception e) {
            Log.e(LOG_TAG, "IO exception loading augmented image database.", e);

            database = new AugmentedImageDatabase(session);
            Bitmap matrixImage = BitmapFactory.decodeResource(getResources(), R.drawable.matrix);
            Bitmap rabbitImage = BitmapFactory.decodeResource(getResources(), R.drawable.rabbit);
            Bitmap math2Image = BitmapFactory.decodeResource(getResources(), R.drawable.math2);
            Bitmap math4Image = BitmapFactory.decodeResource(getResources(), R.drawable.math4);
            // Every image has to have its own unique String identifier
            database.addImage("matrix", matrixImage);
            database.addImage("rabbit", rabbitImage);
            database.addImage("math2", math2Image);
            database.addImage("math4", math4Image);
        }

        config.setAugmentedImageDatabase(database);

        // Check for image detection
        arFragment.setOnAugmentedImageUpdateListener(this::onAugmentedImageTrackingUpdate);
        arFragment.getArSceneView().getScene().addOnUpdateListener(this::onUpdateFrame);



        try (InputStream is = getAssets().open(AR_IMAGE_DATABASE_JSON_FILENAME)) {

            StringBuilder sb = new StringBuilder();
            BufferedReader br = new BufferedReader(new InputStreamReader(is, StandardCharsets.UTF_8));
            String str;
            while ((str = br.readLine()) != null) {
                sb.append(str);
            }
            br.close();

            Gson gson = new Gson();

            Type listType = new TypeToken<ArrayList<ArImageSet>>(){}.getType();
            mArImageSet = gson.fromJson(sb.toString(), listType);

        } catch (Exception ignored) {
        }
    }

    private int testIndex = 0;

    private Mat mIntermediateMat;
    Mat hierarchy;
    List<MatOfPoint> contours;
    private final Comparator<MatOfPoint> mContourComp = (matOfPoint, t1) -> Double.compare(Imgproc.contourArea(t1), Imgproc.contourArea(matOfPoint));

    private void onUpdateFrame(FrameTime frameTime) {

        Frame frame = arFragment.getArSceneView().getArFrame();
        Log.d(LOG_TAG, "onUpdateFrame: frame time(ms) = " + frameTime.getDeltaTime(TimeUnit.MICROSECONDS));

        // If there is no frame or ARCore is not tracking yet, just return.
        if (frame == null || frame.getCamera().getTrackingState() != TrackingState.TRACKING) {
            return;
        }

        // Copy the camera stream to a bitmap
        if (++testIndex % 30 == 0) {

            try (Image image = frame.acquireCameraImage()) {
                if (image.getFormat() != ImageFormat.YUV_420_888) {
                    throw new IllegalArgumentException(
                            "Expected image in YUV_420_888 format, got format " + image.getFormat());
                }

                Log.d(LOG_TAG, "onUpdateFrame: image size = (" + image.getWidth() + "x" + image.getHeight() +")");

                Mat bgrMat = ImageUtils.imageToBgrMat(image);

                Pair<Mat, List<MatOfPoint>> result = mEdgeDetector.contours(bgrMat, testIndex);
                Mat cannyMat = result.first;
                List<MatOfPoint> contours = result.second;

                Imgproc.drawContours(bgrMat,
                        contours,
                        -1, new Scalar(255, 0, 0), 10);

            } catch (Exception e) {
                Log.e(LOG_TAG, "Exception copying image", e);
            }

            testIndex = 0;
        }
    }

    private void onUpdateFrameBackup(FrameTime frameTime) {

        Frame frame = arFragment.getArSceneView().getArFrame();

        Log.d(LOG_TAG, "onUpdateFrame: frame time(ms) = " + frameTime.getDeltaTime(TimeUnit.MICROSECONDS));

        // If there is no frame or ARCore is not tracking yet, just return.
        if (frame == null || frame.getCamera().getTrackingState() != TrackingState.TRACKING) {
            return;
        }

        /*
        Collection<AugmentedImage> updatedAugmentedImages =
                frame.getUpdatedTrackables(AugmentedImage.class);
        for (AugmentedImage augmentedImage : updatedAugmentedImages) {
            switch (augmentedImage.getTrackingState()) {
                case PAUSED:
                    // When an image is in PAUSED state, but the camera is not PAUSED, it has been detected,
                    // but not yet tracked.
                    String text = "Detected Image " + augmentedImage.getIndex();
                    snackbarHelper.showMessage(this, text);
                    break;

                case TRACKING:
                    // Have to switch to UI Thread to update View.
                    break;

                case STOPPED:
                    break;
            }
        }
        */

        // Copy the camera stream to a bitmap

        if (++testIndex % 30 == 0) {
            try (Image image = frame.acquireCameraImage()) {
                if (image.getFormat() != ImageFormat.YUV_420_888) {
                    throw new IllegalArgumentException(
                            "Expected image in YUV_420_888 format, got format " + image.getFormat());
                }

                Log.d(LOG_TAG, "onUpdateFrame: image size = (" + image.getWidth() + "x" + image.getHeight() +")");

                Mat bgrMat = ImageUtils.imageToBgrMat(image);

                Pair<Mat, List<MatOfPoint>> result = mEdgeDetector.contours(bgrMat, testIndex);
                Mat cannyMat = result.first;
                List<MatOfPoint> contours = result.second;

                if (false) {

                    try {
                        // Find ConvexHull including all points in CROP IMAGE
                        List<Point> allPoints = new ArrayList<>();
                        contours.forEach(x -> allPoints.addAll(x.toList()));
                        MatOfInt convexHullPoints = new MatOfInt();
                        Imgproc.convexHull(new MatOfPoint(allPoints.toArray(new Point[0])), convexHullPoints, true);

                        List<Point> convexPoints = new ArrayList<>();
                        for (int index : convexHullPoints.toList()) {
                            convexPoints.add(allPoints.get(index));
                        }
                        MatOfPoint convexPoint = new MatOfPoint(convexPoints.toArray(new Point[0]));
                        contours = new ArrayList<>();
                        contours.add(convexPoint);

                    } catch (CvException cve) {
                        // Ignore
                        contours.clear();
                    }

                } else {

                    List<MatOfPoint> convexContours = new ArrayList<>();

                    contours.forEach(contour -> {

                        List<Point> contourPoints = contour.toList();
                        MatOfInt convexHullPoints = new MatOfInt();
                        Imgproc.convexHull(contour, convexHullPoints, true);

                        List<Point> convexPoints = new ArrayList<>();
                        for (int index : convexHullPoints.toList()) {
                            convexPoints.add(contourPoints.get(index));
                        }
                        MatOfPoint convexContour = new MatOfPoint(convexPoints.toArray(new Point[0]));
                        convexContours.add(convexContour);
                    });

                    contours = convexContours;

                    contours = contours.stream()
                            // .filter(x -> Imgproc.contourArea(x) < 8000)
                            .filter(x -> {
                                MatOfInt convexHullPoints = new MatOfInt();
                                Imgproc.convexHull(x, convexHullPoints, true);
                                if (convexHullPoints.elemSize() < 4) {
                                    return false;
                                }
                                return true;
                            })
                            .filter(x -> {
                                MatOfPoint2f test2f = new MatOfPoint2f(x.toArray());
                                RotatedRect rRect = Imgproc.minAreaRect(test2f);

                                double ratio = rRect.size.width / rRect.size.height;
                                if (ratio < 1) ratio = 1 / ratio;
                                //if (rRect.size.width < 200) return false;
                                //if (rRect.size.height < 200) return false;
                                //if (ratio > 3) return false;
                                return true;
                            })
                            .sorted(mContourComp)
                            //.skip(0)
                            .limit(1)
                            .collect(Collectors.toList());
                }

                // Imgproc.drawContours(bgrMat, contours, -1, new Scalar(0, 0, 255), 10);

                if (contours.size() == 0) {
                    mThresholdIndex++;
                    mThresholdIndex = mThresholdIndex % THRESHOLDS.length;

                    // If failed to find right contours, use all points instead
                    try {
                        // Find ConvexHull including all points in CROP IMAGE
                        List<Point> allPoints = new ArrayList<>();
                        contours.forEach(x -> allPoints.addAll(x.toList()));
                        MatOfInt convexHullPoints = new MatOfInt();
                        Imgproc.convexHull(new MatOfPoint(allPoints.toArray(new Point[0])), convexHullPoints, true);

                        List<Point> convexPoints = new ArrayList<>();
                        for (int index : convexHullPoints.toList()) {
                            convexPoints.add(allPoints.get(index));
                        }
                        MatOfPoint convexPoint = new MatOfPoint(convexPoints.toArray(new Point[0]));
                        contours = new ArrayList<>();
                        contours.add(convexPoint);

                    } catch (CvException cve) {
                        // Ignore
                        contours.clear();
                    }
                }

                List<MatOfPoint> candidates = new ArrayList<>();

                for (MatOfPoint contour : contours) {

                    // Filter 된 Contour 둘러싸고 있는 rotated rect 찾기
                    MatOfPoint2f test2f = new MatOfPoint2f(contour.toArray());

                    RotatedRect rRect = Imgproc.minAreaRect(test2f);

                    // Add more background
                    //rRect.size.width += 6;
                    //rRect.size.height += 6;

                    Point[] rRectPoints = new Point[4];
                    rRect.points(rRectPoints);

                    Log.d(LOG_TAG, "rRect = " + rRect); // 1920 * 1080
                    Log.d(LOG_TAG, "rRect data = " + Arrays.toString(rRectPoints));

                    MatOfPoint test2 = new MatOfPoint();
                    test2.fromArray(rRectPoints);
                    Log.d(LOG_TAG, "test2 = " + test2 + ", element size = " + test2.elemSize());

                    candidates.add(test2);

                    Point lu = new Point(0, 0);
                    Point ru = new Point(640, 0);
                    Point rd = new Point(640, 360);
                    Point ld = new Point(0, 360);
                    MatOfPoint2f hSrc2f = new MatOfPoint2f(test2.toArray());
                    MatOfPoint2f hDst2f = new MatOfPoint2f();
                    hDst2f.fromArray(lu, ru, rd, ld);

                    Mat homoMat = Calib3d.findHomography(hSrc2f, hDst2f);
                    Log.d(LOG_TAG, "homoMat data = " + homoMat);

                    Mat warpMat = new Mat();
                    try {
                        Imgproc.warpPerspective(bgrMat, warpMat, homoMat, new Size(640, 360));
                    } catch (Exception e) {
                        Log.w(LOG_TAG, "Exception e = ", e);
                        continue;
                    }

                    // Log.d(LOG_TAG, "Save Bitmap, then Decode!!!");

                    // Imgproc.polylines(mRgba, candidates, true, new Scalar(255, 0, 0), 10);
                    /*
                    Imgproc.drawContours(bgrMat,
                            candidates,
                            -1, new Scalar(255, 0, 0), 10);
                     */

                    Bitmap bitmap2 = ImageUtils.imageBgrMatToBitmap(warpMat);
                    ImageUtils.SaveBitmapToFile(this, bitmap2);

                    ImgHashBase hashAlgorithm = PHash.create();
                    Mat imageHash = new Mat();
                    hashAlgorithm.compute(warpMat, imageHash);


                    int imageHashSize = (int) (imageHash.total() * imageHash.channels());

                    if (imageHash.depth() == CvType.CV_8U) { //bytes !  <=== right
                        byte[] buffer = new byte[imageHashSize];
                        imageHash.get(0, 0, buffer);
                        Log.d(LOG_TAG, "imageHash (byte) = " + bytesToHex(buffer));

                        // TODO: Compare with ImageHash Database here.
                        long hash = TypeConvertor.bytesToLong(buffer);
                        Optional<ArImageSet> target = mArImageSet.stream().filter(x->x.HasImage(hash)).findFirst();
                        if (target.isPresent()) {
                            ArImageSet found = target.get();
                            Log.d(LOG_TAG, "imageHash found: name = " + found.Name);
                        } else {
                            Log.d(LOG_TAG, "imageHash NOT found");
                        }

                    } else if (imageHash.depth() == CvType.CV_32F) { //float !
                        float[] buffer = new float[imageHashSize];
                        imageHash.get(0, 0, buffer);
                        Log.d(LOG_TAG, "imageHash (float) = " + Arrays.toString(buffer));
                    }

                    //Bitmap bitmap = ImageUtils.imageBgrMatToBitmap(bgrMat);
                    //ImageUtils.SaveBitmapToFile(this, bitmap);
                }

                /*
                ByteBuffer processedImageBytesGrayscale =
                        edgeDetector.detect(
                                image.getWidth(),
                                image.getHeight(),
                                image.getPlanes()[0].getRowStride(),
                                image.getPlanes()[0].getBuffer());

                Bitmap bitmap = Bitmap.createBitmap(image.getWidth(), image.getHeight(),
                        Bitmap.Config.ALPHA_8);
                processedImageBytesGrayscale.rewind();
                bitmap.copyPixelsFromBuffer(processedImageBytesGrayscale);
                */

            } catch (Exception e) {
                Log.e(LOG_TAG, "Exception copying image", e);
            }

            testIndex = 0;
        }
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();

        mIntermediateMat.release();
        hierarchy.release();

        futures.forEach(future -> {
            if (!future.isDone())
                future.cancel(true);
        });

        if (mediaPlayer != null) {
            mediaPlayer.stop();
            mediaPlayer.reset();
        }
    }

    private void loadMatrixModel() {
        futures.add(ModelRenderable.builder()
                .setSource(this, Uri.parse("models/Video.glb"))
                .setIsFilamentGltf(true)
                .build()
                .thenAccept(model -> {
                    //removing shadows for this Renderable
                    model.setShadowCaster(false);
                    model.setShadowReceiver(true);
                    plainVideoModel = model;
                })
                .exceptionally(
                        throwable -> {
                            Toast.makeText(this, "Unable to load renderable", Toast.LENGTH_LONG).show();
                            return null;
                        }));
    }

    private void loadMatrixMaterial() {
        Engine filamentEngine = EngineInstance.getEngine().getFilamentEngine();

        MaterialBuilder.init();
        MaterialBuilder materialBuilder = new MaterialBuilder()
                .platform(MaterialBuilder.Platform.MOBILE)
                .name("External Video Material")
                .require(MaterialBuilder.VertexAttribute.UV0)
                .shading(MaterialBuilder.Shading.UNLIT)
                .doubleSided(true)
                .samplerParameter(MaterialBuilder.SamplerType.SAMPLER_EXTERNAL, MaterialBuilder.SamplerFormat.FLOAT, MaterialBuilder.ParameterPrecision.DEFAULT, "videoTexture")
                .optimization(MaterialBuilder.Optimization.NONE);

        MaterialPackage plainVideoMaterialPackage = materialBuilder
                .blending(MaterialBuilder.BlendingMode.OPAQUE)
                .material("void material(inout MaterialInputs material) {\n" +
                        "    prepareMaterial(material);\n" +
                        "    material.baseColor = texture(materialParams_videoTexture, getUV0()).rgba;\n" +
                        "}\n")
                .build(filamentEngine);
        if (plainVideoMaterialPackage.isValid()) {
            ByteBuffer buffer = plainVideoMaterialPackage.getBuffer();
            futures.add(Material.builder()
                    .setSource(buffer)
                    .build()
                    .thenAccept(material -> {
                        plainVideoMaterial = material;
                    })
                    .exceptionally(
                            throwable -> {
                                Toast.makeText(this, "Unable to load material", Toast.LENGTH_LONG).show();
                                return null;
                            }));
        }
        MaterialBuilder.shutdown();
    }

    public void onAugmentedImageTrackingUpdate(AugmentedImage augmentedImage) {
        // If there are both images already detected, for better CPU usage we do not need scan for them

        Log.d(LOG_TAG, "onAugmentedImageTrackingUpdate(): detected name = " + augmentedImage.getName());

        if (matrixDetected && rabbitDetected && math2Detected && math4Detected) {
            return;
        }

        if (augmentedImage.getTrackingState() == TrackingState.TRACKING
                && augmentedImage.getTrackingMethod() == AugmentedImage.TrackingMethod.FULL_TRACKING) {

            // Setting anchor to the center of Augmented Image
            AnchorNode anchorNode = new AnchorNode(augmentedImage.createAnchor(augmentedImage.getCenterPose()));

            // If matrix video haven't been placed yet and detected image has String identifier of "matrix"
            if (!matrixDetected && augmentedImage.getName().equals("matrix")) {
                matrixDetected = true;
                Toast.makeText(this, "Matrix tag detected", Toast.LENGTH_LONG).show();

                // AnchorNode placed to the detected tag and set it to the real size of the tag
                // This will cause deformation if your AR tag has different aspect ratio than your video
                anchorNode.setWorldScale(new Vector3(augmentedImage.getExtentX(), 1f, augmentedImage.getExtentZ()));
                arFragment.getArSceneView().getScene().addChild(anchorNode);

                TransformableNode videoNode = new TransformableNode(arFragment.getTransformationSystem());
                // For some reason it is shown upside down so this will rotate it correctly
                videoNode.setLocalRotation(Quaternion.axisAngle(new Vector3(0, 1f, 0), 180f));
                anchorNode.addChild(videoNode);

                // Setting texture
                ExternalTexture externalTexture = new ExternalTexture();
                RenderableInstance renderableInstance = videoNode.setRenderable(plainVideoModel);
                renderableInstance.setMaterial(plainVideoMaterial);

                // Setting MediaPLayer
                renderableInstance.getMaterial().setExternalTexture("videoTexture", externalTexture);
                mediaPlayer = MediaPlayer.create(this, R.raw.matrix);
                mediaPlayer.setLooping(true);
                mediaPlayer.setSurface(externalTexture.getSurface());
                mediaPlayer.start();
            }
            // If rabbit model haven't been placed yet and detected image has String identifier of "rabbit"
            // This is also example of model loading and placing at runtime
            if (!rabbitDetected && augmentedImage.getName().equals("rabbit")) {
                rabbitDetected = true;
                Toast.makeText(this, "Rabbit tag detected", Toast.LENGTH_LONG).show();

                // anchorNode.setWorldScale(new Vector3(3.5f, 3.5f, 3.5f));
                // anchorNode.setWorldScale(new Vector3(0.4f, 0.4f, 0.4f));
                anchorNode.setWorldScale(new Vector3(0.03f, 0.03f, 0.03f)); // tyrannosaurus_rex
                // anchorNode.setWorldScale(new Vector3(0.0003f, 0.0003f, 0.0003f)); // f18
                arFragment.getArSceneView().getScene().addChild(anchorNode);

                futures.add(ModelRenderable.builder()
                        .setSource(this, Uri.parse("models/tyrannosaurus_rex.glb"))
                        .setIsFilamentGltf(true)
                        .build()
                        .thenAccept(rabbitModel -> {
                            TransformableNode modelNode = new TransformableNode(arFragment.getTransformationSystem());
                            modelNode.setRenderable(rabbitModel).animate(true).start();
                            anchorNode.addChild(modelNode);
                        })
                        .exceptionally(
                                throwable -> {
                                    Toast.makeText(this, "Unable to load rabbit model", Toast.LENGTH_LONG).show();
                                    return null;
                                }));
            }

            if (!math2Detected && augmentedImage.getName().equals("math2")) {
                math2Detected = true;
                Toast.makeText(this, "Math2 tag detected", Toast.LENGTH_LONG).show();

                // anchorNode.setWorldScale(new Vector3(3.5f, 3.5f, 3.5f));
                // anchorNode.setWorldScale(new Vector3(0.4f, 0.4f, 0.4f));
                // anchorNode.setWorldScale(new Vector3(0.03f, 0.03f, 0.03f)); // tyrannosaurus_rex
                anchorNode.setWorldScale(new Vector3(0.0001f, 0.0001f, 0.0001f)); // f18
                arFragment.getArSceneView().getScene().addChild(anchorNode);

                futures.add(ModelRenderable.builder()
                        .setSource(this, Uri.parse("models/f18.glb"))
                        .setIsFilamentGltf(true)
                        .build()
                        .thenAccept(rabbitModel -> {
                            TransformableNode modelNode = new TransformableNode(arFragment.getTransformationSystem());
                            modelNode.setRenderable(rabbitModel).animate(true).start();
                            anchorNode.addChild(modelNode);
                        })
                        .exceptionally(
                                throwable -> {
                                    Toast.makeText(this, "Unable to load f18 model", Toast.LENGTH_LONG).show();
                                    return null;
                                }));
            }

            if (!math4Detected && augmentedImage.getName().equals("math4")) {
                math4Detected = true;
                Toast.makeText(this, "Math4 tag detected", Toast.LENGTH_LONG).show();

                // anchorNode.setWorldScale(new Vector3(3.5f, 3.5f, 3.5f));
                anchorNode.setWorldScale(new Vector3(0.2f, 0.2f, 0.2f));
                // anchorNode.setWorldScale(new Vector3(0.03f, 0.03f, 0.03f)); // tyrannosaurus_rex
                // anchorNode.setWorldScale(new Vector3(0.0003f, 0.0003f, 0.0003f)); // f18
                arFragment.getArSceneView().getScene().addChild(anchorNode);

                futures.add(ModelRenderable.builder()
                        .setSource(this, Uri.parse("models/shiba.glb"))
                        .setIsFilamentGltf(true)
                        .build()
                        .thenAccept(rabbitModel -> {
                            TransformableNode modelNode = new TransformableNode(arFragment.getTransformationSystem());
                            modelNode.setRenderable(rabbitModel).animate(true).start();
                            anchorNode.addChild(modelNode);
                        })
                        .exceptionally(
                                throwable -> {
                                    Toast.makeText(this, "Unable to load shiba model", Toast.LENGTH_LONG).show();
                                    return null;
                                }));
            }
        }
        if (matrixDetected && rabbitDetected && math2Detected && math4Detected) {
            arFragment.getInstructionsController().setEnabled(
                    InstructionsController.TYPE_AUGMENTED_IMAGE_SCAN, false);
        }
    }


    private static final int[] THRESHOLDS = {170, 190, 210};
    private int mThresholdIndex = 0;

    private Mat doCanny(Mat frame) {
        // init
        Mat grayImage = new Mat();
        Mat thresholdMat = new Mat();
        Mat BlurredMat = new Mat();
        Mat rectKernel = new Mat();
        Mat dilatedMat = new Mat();

        // convert to grayscale
        Imgproc.cvtColor(frame, grayImage, Imgproc.COLOR_BGR2GRAY);

        Imgproc.threshold(grayImage, thresholdMat, THRESHOLDS[mThresholdIndex], 255, Imgproc.THRESH_BINARY);

        // reduce noise with a 3x3 kernel
        // Imgproc.GaussianBlur(thresholdMat, BlurredMat, new Size(3, 3), 0, 0, Core.BORDER_DEFAULT);
        // Imgproc.blur(grayImage, BlurredMat, new Size(3, 3));
        BlurredMat = thresholdMat;

        rectKernel = Imgproc.getStructuringElement(Imgproc.MORPH_RECT, new Size(3, 3));
        // Imgproc.dilate (BlurredMat, dilatedMat, rectKernel, new Point (0,0), 2);
        Imgproc.morphologyEx(BlurredMat, dilatedMat, Imgproc.MORPH_CLOSE, rectKernel);  // Use closing instead of dilate

        // canny detector, with ratio of lower:upper threshold of 3:1
        Imgproc.Canny(dilatedMat, dilatedMat, 50, 100, 3, false);

        // using Canny's output as a mask, display the result
        Mat dest = new Mat();
        Core.add(dest, Scalar.all(0), dest);
        frame.copyTo(dest, dilatedMat);

        return dilatedMat;
    }

    private static final char[] HEX_ARRAY = "0123456789ABCDEF".toCharArray();
    public static String bytesToHex(byte[] bytes) {
        char[] hexChars = new char[bytes.length * 2];
        for (int j = 0; j < bytes.length; j++) {
            int v = bytes[j] & 0xFF;
            hexChars[j * 2] = HEX_ARRAY[v >>> 4];
            hexChars[j * 2 + 1] = HEX_ARRAY[v & 0x0F];
        }
        return new String(hexChars);
    }

}